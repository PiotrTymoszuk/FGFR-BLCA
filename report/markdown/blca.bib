@article{Wickham2019,
abstract = {At a high level, the tidyverse is a language for solving data science challenges with R code. Its primary goal is to facilitate a conversation between a human and a computer about data. Less abstractly, the tidyverse is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next. The tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming. We expect that almost every project will use multiple domain-specific packages outside of the tidyverse: our goal is to provide tooling for the most common challenges; not to solve every possible problem. Notably, the tidyverse doesn't include tools for statistical modelling or communication. These toolkits are critical for data science, but are so large that they merit separate treatment. The tidyverse package allows users to install all tidyverse packages with a single command. There are a number of projects that are similar in scope to the tidyverse. The closest is perhaps Bioconductor (Gentleman et al., 2004; Huber et al., 2015), which provides an ecosystem of packages that support the analysis of high-throughput genomic data.},
author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy and Fran{\c{c}}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas and Miller, Evan and Bache, Stephan and M{\"{u}}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
doi = {10.21105/joss.01686},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf:pdf},
issn = {2475-9066},
journal = {Journal of Open Source Software},
month = {nov},
number = {43},
pages = {1686},
publisher = {The Open Journal},
title = {{Welcome to the Tidyverse}},
volume = {4},
year = {2019}
}
@misc{Gohel2022,
abstract = {Create pretty tables for 'HTML', 'PDF', 'Microsoft Word' and 'Microsoft PowerPoint' documents from 'R Markdown'. Functions are provided to let users create tables, modify and format their content. It also extends package 'officer' that does not contain any feature for customized tabular reporting.},
author = {Gohel, David},
title = {{flextable: Functions for Tabular Reporting}},
url = {https://cran.r-project.org/web/packages/flextable/index.html},
year = {2022}
}
@misc{Allaire2022,
abstract = {Convert R Markdown documents into a variety of formats.},
author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe},
title = {{rmarkdown: Dynamic Documents for R}},
url = {https://cran.r-project.org/web/packages/rmarkdown/index.html},
year = {2022}
}
@incollection{Venables2002,
author = {Venables, Bill and Ripley, B},
booktitle = {Springer},
doi = {10.1007/b97626},
title = {{Modern Applied Statistics With S}},
year = {2002}
}
@article{Garson1991,
author = {Garson, G. David},
doi = {10.5555/129449.129452},
journal = {AI Expert},
number = {4},
pages = {47 --51},
title = {{Interpreting neural-network connection weights}},
url = {https://www.scinapse.io/papers/1833005471},
volume = {6},
year = {1991}
}
@article{Wood2017,
abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models. The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book's R data package gamair, to enable use as a course text or for self-study.},
author = {Wood, Simon N.},
doi = {10.1201/9781315370279/GENERALIZED-ADDITIVE-MODELS-SIMON-WOOD},
isbn = {9781498728348},
journal = {Generalized Additive Models: An Introduction with R, Second Edition},
month = {jan},
pages = {1--476},
publisher = {CRC Press},
title = {{Generalized additive models: An introduction with R, second edition}},
url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315370279/generalized-additive-models-simon-wood},
year = {2017}
}
@misc{Henry2022,
abstract = {A toolbox for working with base types, core R features like the condition system, and core 'Tidyverse' features like tidy evaluation.},
author = {Henry, Lionel and Wickham, Hadley.},
title = {{rlang: Functions for Base Types and Core R and 'Tidyverse' Features}},
url = {https://cran.r-project.org/web/packages/rlang/index.html},
year = {2022}
}
@article{Huber2011,
abstract = {IntroductionThe term ” robust” was introduced into the statistical literature by Box (1953). By then, robust methods such as trimmed means, had been in sporadic use for well over a century, see for example Anonymous (1821). However, Tukey (1960) was the first person to recognize the extreme sensitivity of some conventional statistical procedures to seemingly minor deviations from the assumptions, and to give an eye-opening example. His example, and his realization that statistical methods optimized for the conventional Gaussian model are unstable under small perturbations were crucial for the subsequent theoretical developments initiated by Huber (1964) and Hampel (1968).In the 1960s robust methods still were considered ” dirty” by most. Therefore, to promote their reception in the statistical community it was crucial to mathematize the approach: one had to prove optimality properties, as was done by Huber's minimax results (1964, 1965, 1968), and to give a formal definition ...},
author = {Huber, Peter J.},
doi = {10.1007/978-3-642-04898-2_594},
journal = {International Encyclopedia of Statistical Science},
pages = {1248--1251},
publisher = {Springer, Berlin, Heidelberg},
title = {{Robust Statistics}},
url = {https://link.springer.com/referenceworkentry/10.1007/978-3-642-04898-2_594},
year = {2011}
}
@article{Strobl2009,
abstract = {Recursive partitioning methods have become popular and widely used tools for nonparametric regression and classification in many scientific fields. Especially random forests, which can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine, and bioinformatics within the past few years. High-dimensional problems are common not only in genetics, but also in some areas of psychological research, where only a few subjects can be measured because of time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications and to provide descriptive variable importance measures reflecting the impact of each variable in both main effects and interactions. The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high-dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application. Application of the methods is illustrated with freely available implementations in the R system for statistical computing. {\textcopyright} 2009 American Psychological Association.},
author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
doi = {10.1037/A0016973},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobl, Malley, Tutz - 2009 - An Introduction to Recursive Partitioning Rationale, Application and Characteristics of Classification and.pdf:pdf},
issn = {1082989X},
journal = {Psychological methods},
keywords = {classification,prediction,regression,variable importance},
month = {dec},
number = {4},
pages = {323},
pmid = {19968396},
publisher = {NIH Public Access},
title = {{An Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests}},
url = {/pmc/articles/PMC2927982/ /pmc/articles/PMC2927982/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/},
volume = {14},
year = {2009}
}
@misc{Yan2021,
abstract = {An easy-to-use way to draw pretty venn diagram by 'ggplot2'.},
author = {Yan, Linlin},
month = {jun},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{ggvenn: Draw Venn Diagram by 'ggplot2'}},
url = {https://cran.r-project.org/package=ggvenn},
year = {2021}
}
@misc{Mangiafico2022,
abstract = {Functions and datasets to support "Summary and Analysis of Extension Program Evaluation in R" and "An R Companion for the Handbook of Biological Statistics". Vignettes are available at <http://rcompanion.org>.},
author = {Mangiafico, Salvatore},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{rcompanion: Functions to Support Extension Education Program Evaluation}},
url = {https://cran.r-project.org/package=rcompanion},
year = {2022}
}
@article{Breiman2017,
abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
doi = {10.1201/9781315139470/CLASSIFICATION-REGRESSION-TREES-LEO-BREIMAN},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman et al. - 2017 - Classification and regression trees.pdf:pdf},
isbn = {9781351460491},
journal = {Classification and Regression Trees},
month = {jan},
pages = {1--358},
publisher = {CRC Press},
title = {{Classification and regression trees}},
url = {https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman},
year = {2017}
}
@book{Ripley2014,
abstract = {Ripley brings together two crucial ideas in pattern recognition: statistical methods and machine learning via neural networks. He brings unifying principles to the fore, and reviews the state of the subject. Ripley also includes many examples to illustrate real problems in pattern recognition and how to overcome them.},
author = {Ripley, Brian D.},
booktitle = {Pattern Recognition and Neural Networks},
doi = {10.1017/CBO9780511812651},
isbn = {9780511812651},
month = {jan},
pages = {1--403},
publisher = {Cambridge University Press},
title = {{Pattern recognition and neural networks}},
url = {https://www.cambridge.org/core/books/pattern-recognition-and-neural-networks/4E038249C9BAA06C8F4EE6F044D09C5C},
year = {2014}
}
@article{Kuhn2008,
abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
author = {Kuhn, Max},
doi = {10.18637/jss.v028.i05},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Model building,NetWorkSpaces,Parallel processing,R,Tuning parameters},
number = {5},
pages = {1--26},
publisher = {American Statistical Association},
title = {{Building predictive models in R using the caret package}},
volume = {28},
year = {2008}
}
@misc{Xie2022,
abstract = {Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.},
author = {Xie, Yihui},
title = {{knitr: A General-Purpose Package for Dynamic Report Generation in R}},
url = {https://cran.r-project.org/web/packages/knitr/index.html},
year = {2022}
}
@misc{Hothorn2022,
abstract = {A computational toolbox for recursive partitioning. The core of the package is ctree(), an implementation of conditional inference trees which embed tree-structured regression models into a well defined theory of conditional inference procedures. This non-parametric class of regression trees is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Based on conditional inference trees, cforest() provides an implementation of Breiman's random forests. The function mob() implements an algorithm for recursive partitioning based on parametric models (e.g. linear models, GLMs or survival regression) employing parameter instability tests for split selection. Extensible functionality for visualizing tree-structured regression models is available. The methods are described in Hothorn et al. (2006) <doi:10.1198/106186006X133933>, Zeileis et al. (2008) <doi:10.1198/106186008X319331> and Strobl et al. (2007) <doi:10.1186/1471-2105-8-25>.},
author = {Hothorn, Thorsten and Hornik, Kurt and Strobl, Carolin and Zeileis, Achim},
title = {{party: A Laboratory for Recursive Partytioning}},
url = {https://cran.r-project.org/web/packages/party/index.html},
year = {2022}
}
@article{McHugh2012,
abstract = {The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. While there have been a variety of methods to measure interrater reliability, traditionally it was measured as percent agreement, calculated as the number of agreement scores divided by the total number of scores. In 1960, Jacob Cohen critiqued use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen's kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. Like most correlation statistics, the kappa can range from -1 to +1. While the kappa is one of the most commonly used statistics to test interrater reliability, it has limitations. Judgments about what level of kappa should be acceptable for health research are questioned. Cohen's suggested interpretation may be too lenient for health related studies because it implies that a score as low as 0.41 might be acceptable. Kappa and percent agreement are compared, and levels for both kappa and percent agreement that should be demanded in healthcare studies are suggested.},
author = {McHugh, Mary L.},
doi = {10.11613/bm.2012.031},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McHugh - 2012 - Interrater reliability the kappa statistic(2).pdf:pdf},
issn = {13300962},
journal = {Biochemia Medica},
keywords = {Interrater,Kappa,Rater,Reliability},
number = {3},
pages = {276},
pmid = {23092060},
publisher = {Croatian Society for Medical Biochemistry and Laboratory Medicine},
title = {{Interrater reliability: the kappa statistic}},
url = {/pmc/articles/PMC3900052/ /pmc/articles/PMC3900052/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/},
volume = {22},
year = {2012}
}
@article{Sahanic2023,
author = {Sahanic, Sabina and Tymoszuk, Piotr and Luger, Anna K. and H{\"{u}}fner, Katharina and Boehm, Anna and Pizzini, Alex and Schwabl, Christoph and Koppelst{\"{a}}tter, Sabine and Kurz, Katharina and Asshoff, Malte and Mosheimer-Feistritzer, Birgit and Coen, Maximilian and Pfeifer, Bernhard and Rass, Verena and Egger, Alexander and H{\"{o}}rmann, Gregor and Sperner-Unterweger, Barbara and Helbok, Raimund and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Widmann, Gerlig and Tancevski, Ivan and Sonnweber, Thomas and L{\"{o}}ffler-Ragg, Judith},
doi = {10.1183/23120541.00317-2022},
issn = {2312-0541},
journal = {ERJ open research},
keywords = {Judith L{\"{o}}ffler-Ragg,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC10030059,Piotr Tymoszuk,PubMed Abstract,Sabina Sahanic,doi:10.1183/23120541.00317-2022,pmid:36960350},
month = {mar},
number = {2},
pages = {00317--2022},
pmid = {36960350},
publisher = {ERJ Open Res},
title = {{COVID-19 and its continuing burden after 12 months: a longitudinal observational prospective multicentre trial}},
url = {https://pubmed.ncbi.nlm.nih.gov/36960350/},
volume = {9},
year = {2023}
}
@book{Wilke2019,
abstract = {First edition. Intro; Copyright; Table of Contents; Preface; Thoughts on Graphing Software and Figure-Preparation Pipelines; Conventions Used in This Book; Using Code Examples; O'Reilly Online Learning; How to Contact Us; Acknowledgments; Chapter 1. Introduction; Ugly, Bad, and Wrong Figures; Part I. From Data to Visualization; Chapter 2. Visualizing Data: Mapping Data onto Aesthetics; Aesthetics and Types of Data; Scales Map Data Values onto Aesthetics; Chapter 3. Coordinate Systems and Axes; Cartesian Coordinates; Nonlinear Axes; Coordinate Systems with Curved Axes; Chapter 4. Color Scales Color as a Tool to DistinguishColor to Represent Data Values; Color as a Tool to Highlight; Chapter 5. Directory of Visualizations; Amounts; Distributions; Proportions; x-y relationships; Geospatial Data; Uncertainty; Chapter 6. Visualizing Amounts; Bar Plots; Grouped and Stacked Bars; Dot Plots and Heatmaps; Chapter 7. Visualizing Distributions: Histograms and Density Plots; Visualizing a Single Distribution; Visualizing Multiple Distributions at the Same Time; Chapter 8. Visualizing Distributions: Empirical Cumulative Distribution Functions and Q-Q Plots Empirical Cumulative Distribution FunctionsHighly Skewed Distributions; Quantile-Quantile Plots; Chapter 9. Visualizing Many Distributions at Once; Visualizing Distributions Along the Vertical Axis; Visualizing Distributions Along the Horizontal Axis; Chapter 10. Visualizing Proportions; A Case for Pie Charts; A Case for Side-by-Side Bars; A Case for Stacked Bars and Stacked Densities; Visualizing Proportions Separately as Parts of the Total; Chapter 11. Visualizing Nested Proportions; Nested Proportions Gone Wrong; Mosaic Plots and Treemaps; Nested Pies; Parallel Sets Chapter 12. Visualizing Associations Among Two or More Quantitative VariablesScatterplots; Correlograms; Dimension Reduction; Paired Data; Chapter 13. Visualizing Time Series and Other Functions of an Independent Variable; Individual Time Series; Multiple Time Series and Dose-Response Curves; Time Series of Two or More Response Variables; Chapter 14. Visualizing Trends; Smoothing; Showing Trends with a Defined Functional Form; Detrending and Time-Series Decomposition; Chapter 15. Visualizing Geospatial Data; Projections; Layers; Choropleth Mapping; Cartograms Chapter 16. Visualizing UncertaintyFraming Probabilities as Frequencies; Visualizing the Uncertainty of Point Estimates; Visualizing the Uncertainty of Curve Fits; Hypothetical Outcome Plots; Part II. Principles of Figure Design; Chapter 17. The Principle of Proportional Ink; Visualizations Along Linear Axes; Visualizations Along Logarithmic Axes; Direct Area Visualizations; Chapter 18. Handling Overlapping Points; Partial Transparency and Jittering; 2D Histograms; Contour Lines; Chapter 19. Common Pitfalls of Color Use; Encoding Too Much or Irrelevant Information},
address = {Sebastopol},
author = {Wilke, Claus O},
booktitle = {O'Reilly Media},
edition = {1},
isbn = {1492031089},
pages = {389},
publisher = {O'Reilly Media},
title = {{Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures}},
year = {2019}
}
@article{Sonnweber2020,
abstract = {Background: After the 2002/2003 severe acute respiratory syndrome outbreak, 30% of survivors exhibited persisting structural pulmonary abnormalities. The long-term pulmonary sequelae of coronavirus disease 2019 (COVID-19) are yet unknown, and comprehensive clinical follow-up data are lacking. Methods: In this prospective, multicentre, observational study, we systematically evaluated the cardiopulmonary damage in subjects recovering from COVID-19 at 60 and 100 days after confirmed diagnosis. We conducted a detailed questionnaire, clinical examination, laboratory testing, lung function analysis, echocardiography and thoracic low-dose computed tomography (CT). Results: Data from 145 COVID-19 patients were evaluated, and 41% of all subjects exhibited persistent symptoms 100 days after COVID-19 onset, with dyspnoea being most frequent (36%). Accordingly, patients still displayed an impaired lung function, with a reduced diffusing capacity in 21% of the cohort being the most prominent finding. Cardiac impairment, including a reduced left ventricular function or signs of pulmonary hypertension, was only present in a minority of subjects. CT scans unveiled persisting lung pathologies in 63% of patients, mainly consisting of bilateral ground-glass opacities and/or reticulation in the lower lung lobes, without radiological signs of pulmonary fibrosis. Sequential follow-up evaluations at 60 and 100 days after COVID-19 onset demonstrated a vast improvement of symptoms and CT abnormalities over time. Conclusion: A relevant percentage of post-COVID-19 patients presented with persisting symptoms and lung function impairment along with radiological pulmonary abnormalities >100 days after the diagnosis of COVID-19. However, our results indicate a significant improvement in symptoms and cardiopulmonary status over time.},
author = {Sonnweber, Thomas and Sahanic, Sabina and Pizzini, Alex and Luger, Anna and Schwabl, Christoph and Sonnweber, Bettina and Kurz, Katharina and Koppelst{\"{a}}tter, Sabine and Haschka, David and Petzer, Verena and Boehm, Anna and Aichner, Magdalena and Tymoszuk, Piotr and Lener, Daniela and Theurl, Markus and Lorsbach-K{\"{o}}hler, Almut and Tancevski, Amra and Schapfl, Anna and Schaber, Marc and Hilbe, Richard and Nairz, Manfred and Puchner, Bernhard and H{\"{u}}ttenberger, Doris and Tschurtschenthaler, Christoph and A{\ss}hoff, Malte and Peer, Andreas and Hartig, Frank and Bellmann, Romuald and Joannidis, Michael and Gollmann-Tepek{\"{o}}yl{\"{u}}, Can and Holfeld, Johannes and Feuchtner, Gudrun and Egger, Alexander and Hoermann, Gregor and Schroll, Andrea and Fritsche, Gernot and Wildner, Sophie and Bellmann-Weiler, Rosa and Kirchmair, Rudolf and Helbok, Raimund and Prosch, Helmut and Rieder, Dietmar and Trajanoski, Zlatko and Kronenberg, Florian and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Widmann, Gerlig and L{\"{o}}ffler-Ragg, Judith and Tancevski, Ivan},
doi = {10.1183/13993003.03481-2020},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sonnweber et al. - 2020 - Cardiopulmonary recovery after COVID-19 - an observational prospective multi-center trial.pdf:pdf},
issn = {13993003},
journal = {European Respiratory Journal},
keywords = {Ivan Tancevski,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC7736754,PubMed Abstract,Sabina Sahanic,Thomas Sonnweber,doi:10.1183/13993003.03481-2020,pmid:33303539},
month = {dec},
number = {4},
pmid = {33303539},
publisher = {Eur Respir J},
title = {{Cardiopulmonary recovery after COVID-19: An observational prospective multicentre trial}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/33303539},
volume = {57},
year = {2021}
}
@misc{Hansell2008,
abstract = {Members of the Fleischner Society compiled a glossary of terms for thoracic imaging that replaces previous glossaries published in 1984 and 1996 for thoracic radiography and computed tomography (CT), respectively. The need to update the previous versions came from the recognition that new words have emerged, others have become obsolete, and the meaning of some terms has changed. Brief descriptions of some diseases are included, and pictorial examples (chest radiographs and CT scans) are provided for the majority of terms. {\textcopyright} RSNA, 2008.},
author = {Hansell, David M. and Bankier, Alexander A. and MacMahon, Heber and McLoud, Theresa C. and M{\"{u}}ller, Nestor L. and Remy, Jacques},
booktitle = {Radiology},
doi = {10.1148/radiol.2462070712},
issn = {00338419},
month = {mar},
number = {3},
pages = {697--722},
pmid = {18195376},
publisher = {Radiological Society of North America},
title = {{Fleischner Society: Glossary of terms for thoracic imaging}},
url = {https://pubs.rsna.org/doi/10.1148/radiol.2462070712},
volume = {246},
year = {2008}
}
@article{Cohen2013,
abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
author = {Cohen, Jacob},
doi = {10.4324/9780203771587},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen - 2013 - Statistical Power Analysis for the Behavioral Sciences.pdf:pdf},
isbn = {9780203771587},
journal = {Statistical Power Analysis for the Behavioral Sciences},
month = {may},
publisher = {Routledge},
title = {{Statistical Power Analysis for the Behavioral Sciences}},
url = {https://www.taylorfrancis.com/books/mono/10.4324/9780203771587/statistical-power-analysis-behavioral-sciences-jacob-cohen},
year = {2013}
}
@misc{Mayer2023,
abstract = {Visualizations for SHAP (SHapley Additive exPlanations), such as waterfall plots, force plots, various types of importance plots, dependence plots, and interaction plots. These plots act on a 'shapviz' object created from a matrix of SHAP values and a corresponding feature dataset. Wrappers for the R packages 'xgboost', 'lightgbm', 'fastshap', 'shapr', 'h2o', 'treeshap', 'DALEX', and 'kernelshap' are added for convenience. By separating visualization and computation, it is possible to display factor variables in graphs, even if the SHAP values are calculated by a model that requires numerical features. The plots are inspired by those provided by the 'shap' package in Python, but there is no dependency on it.},
author = {Mayer, Michael and Stando, Adrian},
title = {{shapviz: SHAP Visualizations}},
url = {https://cran.r-project.org/web/packages/shapviz/index.html},
year = {2023}
}
@misc{Gagolewski2021,
abstract = {A collection of character string/text/natural language processing tools for pattern searching (e.g., with 'Java'-like regular expressions or the 'Unicode' collation algorithm), random string generation, case mapping, string transliteration, concatenation, sorting, padding, wrapping, Unicode normalisation, date-time formatting and parsing, and many more. They are fast, consistent, convenient, and - thanks to 'ICU' (International Components for Unicode) - portable across all locales and platforms.},
author = {Gagolewski, Marek and Tartanus, Bartek},
title = {{Package 'stringi'}},
url = {https://cran.r-project.org/web/packages/stringi/index.html http://cran.ism.ac.jp/web/packages/stringi/stringi.pdf},
year = {2021}
}
@article{Benjamini1995,
abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses- the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
author = {Benjamini, Yoav and Hochberg, Yosef},
doi = {10.1111/j.2517-6161.1995.tb02031.x},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {bonferroni‐type procedures,familywise error rate,multiple‐comparison procedures,p‐values},
month = {jan},
number = {1},
pages = {289--300},
publisher = {Wiley},
title = {{Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing}},
volume = {57},
year = {1995}
}
@misc{Greenwell2022,
abstract = {An implementation of extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine. Includes regression methods for least squares, absolute loss, t-distribution loss, quantile regression, logistic, multinomial logistic, Poisson, Cox proportional hazards partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway.},
author = {Greenwell, Brandon and Boehmke, Bradley and Cunningham, Jay and Developers, GBM},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{gbm: Generalized Boosted Regression Models}},
url = {https://cran.r-project.org/package=gbm},
year = {2022}
}
@book{Xie2016,
abstract = {bookdown: Authoring Books and Technical Documents with R Markdown presents a much easier way to write books and technical publications than traditional tools such as LaTeX and Word. The bookdown package inherits the simplicity of syntax and flexibility for data analysis from R Markdown, and extends R Markdown for technical writing, so that you can make better use of document elements such as figures, tables, equations, theorems, citations, and references. Similar to LaTeX, you can number and cross-reference these elements with bookdown. Your document can even include live examples so readers can interact with them while reading the book. The book can be rendered to multiple output formats, including LaTeX/PDF, HTML, EPUB, and Word, thus making it easy to put your documents online. The style and theme of these output formats can be customized. We used books and R primarily for examples in this book, but bookdown is not only for books or R. Most features introduced in this book also apply to other types of publications: journal papers, reports, dissertations, course handouts, study notes, and even novels. You do not have to use R, either. Other choices of computing languages include Python, C, C plus plus, SQL, Bash, Stan, JavaScript, and so on, although R is best supported. You can also leave out computing, for example, to write a fiction. This book itself is an example of publishing with bookdown and R Markdown, and its source is fully available on GitHub.},
author = {Xie, Yihui},
booktitle = {Bookdown: Authoring Books and Technical Documents with R Markdown},
doi = {10.1201/9781315204963},
isbn = {9781351792608},
pages = {1--113},
title = {{Bookdown: Authoring books and technical documents with R Markdown}},
year = {2016}
}
@misc{Wood2023,
abstract = {Generalized additive (mixed) models, some of their extensions and other generalized ridge regression with multiple smoothing parameter estimation by (Restricted) Marginal Likelihood, Generalized Cross Validation and similar, or using iterated nested Laplace approximation for fully Bayesian inference. See Wood (2017) <doi:10.1201/9781315370279> for an overview. Includes a gam() function, a wide variety of smoothers, 'JAGS' support and distributions beyond the exponential family.},
author = {Wood, Simon},
title = {{mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation}},
url = {https://cran.r-project.org/web/packages/mgcv/index.html},
year = {2023}
}
@misc{Slowikowski2023,
abstract = {Provides text and label geoms for 'ggplot2' that help to avoid overlapping text labels. Labels repel away from each other and away from the data points.},
author = {Slowikowski, Kamil and Shep, Alicia and Hughes, Sean and Dang, Trung Kien and Lukauskas, Saulius and Irisson, Jean-Olivier and Kamvar, Zhian N and Ryan, Thompson and Dervieux, Christophe and Yutani, Hiroaki and Gramme, Pierre and Abdol, Amir Masoud and Barrett, Malcolm and Cannoodt, Robrecht and Krassowski, Micha{\l} and Chirico, Michael and Aphalo, Perdo},
title = {{ggrepel: Automatically Position Non-Overlapping Text Labels with 'ggplot2'}},
url = {https://cran.r-project.org/web/packages/ggrepel/index.html},
year = {2023}
}
@misc{Ripley2022,
abstract = {Functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S" (4th edition, 2002).},
author = {Ripley, Brian},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{MASS: Support Functions and Datasets for Venables and Ripley's MASS}},
url = {https://cran.r-project.org/package=MASS},
year = {2022}
}
@misc{Therneau2022,
abstract = {Recursive partitioning for classification, regression and survival trees. An implementation of most of the functionality of the 1984 book by Breiman, Friedman, Olshen and Stone.},
author = {Therneau, Terry M. and Atkinson, Beth and Ripley, Brian D.},
title = {{rpart: Recursive Partitioning and Regression Trees}},
url = {https://cran.r-project.org/web/packages/rpart/index.html},
year = {2022}
}
@misc{Goldstein-Greenwood2021,
author = {Goldstein-Greenwood, Jacob},
title = {{A Brief on Brier Scores | UVA Library}},
url = {https://library.virginia.edu/data/articles/a-brief-on-brier-scores},
urldate = {2023-09-05},
year = {2021}
}
@article{Hopkins1954,
abstract = {The method depends on linear measurements between random points and adjacent individuals, and between adjacent pairs of individuals. Its results compare favourably with those of the current methods when tested on synthetic and natural populations. The method is quicker than the quadrat methods and is especially useful for analysing the distribution of trees. {\textcopyright} 1954 Oxford University Press.},
author = {Hopkins, Brian and Skellam, J. G.},
doi = {10.1093/OXFORDJOURNALS.AOB.A083391},
issn = {0305-7364},
journal = {Annals of Botany},
month = {apr},
number = {2},
pages = {213--227},
publisher = {Oxford Academic},
title = {{A New Method for determining the Type of Distribution of Plant Individuals}},
url = {https://dx.doi.org/10.1093/oxfordjournals.aob.a083391},
volume = {18},
year = {1954}
}
@article{Wright2017,
abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
archivePrefix = {arXiv},
arxivId = {1508.04409},
author = {Wright, Marvin N. and Ziegler, Andreas},
doi = {10.18637/JSS.V077.I01},
eprint = {1508.04409},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright, Ziegler - 2017 - ranger A Fast Implementation of Random Forests for High Dimensional Data in C and R.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {R,Rcpp,classification,machine learning,random forests,recursive partitioning,survival analysis},
month = {mar},
number = {1},
pages = {1--17},
publisher = {American Statistical Association},
title = {{ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v077i01},
volume = {77},
year = {2017}
}
@misc{Signorell2022,
abstract = {A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.},
author = {Signorell, Andri},
month = {oct},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{DescTools: Tools for Descriptive Statistics}},
url = {https://cran.r-project.org/package=DescTools},
year = {2022}
}
@article{Luger2022,
abstract = {Background: The long-term pulmonary sequelae of COVID-19 is not well known. Purpose: To characterize patterns and rates of improvement of chest CT abnormalities 1 year after COVID-19 pneumonia. Materials and Methods: This was a secondary analysis of a prospective, multicenter observational cohort study conducted from April 29 to August 12, 2020, to assess pulmonary abnormalities at chest CT approximately 2, 3, and 6 months and 1 year after onset of COVID-19 symptoms. Pulmonary findings were graded for each lung lobe using a qualitative CT severity score (CTSS) ranging from 0 (normal) to 25 (all lobes involved). The association of demographic and clinical factors with CT abnormalities after 1 year was assessed with logistic regression. The rate of change of the CTSS at follow-up CT was investigated by using the Friedmann test. Results: Of 142 enrolled participants, 91 underwent a 1-year follow-up CT examination and were included in the analysis (mean age, 59 years ± 13 [SD]; 35 women [38%]). In 49 of 91 (54%) participants, CT abnormalities were observed: 31 of 91 (34%) participants showed subtle subpleural reticulation, ground-glass opacities, or both, and 18 of 91 (20%) participants had extensive ground-glass opacities, reticulations, bronchial dilation, microcystic changes, or a combination thereof. At multivariable analysis, age of more than 60 years (odds ratio [OR], 5.8; 95% CI: 1.7, 24; P = .009), critical COVID-19 severity (OR, 29; 95% CI: 4.8, 280; P < .001), and male sex (OR, 8.9; 95% CI: 2.6, 36; P < .001) were associated with persistent CT abnormalities at 1-year follow-up. Reduction of CTSS was observed in participants at subsequent follow-up CT (P < .001); during the study period, 49% (69 of 142) of participants had complete resolution of CT abnormalities. Thirty-one of 49 (63%) participants with CT abnormalities showed no further improvement after 6 months. Conclusion: Long-term CT abnormalities were common 1 year after COVID-19 pneumonia.},
author = {Luger, Anna K. and Sonnweber, Thomas and Gruber, Leonhard and Schwabl, Christoph and Cima, Katharina and Tymoszuk, Piotr and Gerstner, Anna K. and Pizzini, Alex and Sahanic, Sabina and Boehm, Anna and Coen, Maximilian and Strolz, Carola J. and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Kirchmair, Rudolf and Feuchtner, Gudrun M. and Prosch, Helmut and Tancevski, Ivan and L{\"{o}}ffler-Ragg, Judith and Widmann, Gerlig},
doi = {10.1148/radiol.211670},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luger et al. - 2022 - Chest CT of Lung Injury 1 Year after COVID-19 Pneumonia The CovILD Study.pdf:pdf},
issn = {15271315},
journal = {Radiology},
keywords = {Anna K Luger,Gerlig Widmann,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Thomas Sonnweber,doi:10.1148/radiol.211670,pmid:35348379},
month = {mar},
number = {2},
pages = {462--470},
pmid = {35348379},
publisher = {Radiology},
title = {{Chest CT of Lung Injury 1 Year after COVID-19 Pneumonia: The CovILD Study}},
url = {https://pubmed.ncbi.nlm.nih.gov/35348379/},
volume = {304},
year = {2022}
}
@article{Lopez-Raton2014,
abstract = {Continuous diagnostic tests are often used for discriminating between healthy and diseased populations. For the clinical application of such tests, it is useful to select a cutpoint or discrimination value c that deffnes positive and negative test results. In general, individuals with a diagnostic test value of c or higher are classiffed as diseased. Several search strategies have been proposed for choosing optimal cutpoints in diagnostic tests, depending on the underlying reason for this choice. This paper introduces an R package, known as OptimalCutpoints, for selecting optimal cutpoints in diagnostic tests. It incorporates criteria that take the costs of the different diagnostic decisions into account, as well as the prevalence of the target disease and several methods based on measures of diagnostic test accuracy. Moreover, it enables optimal levels to be calculated according to levels of given (categorical) covariates. While the numerical output includes the optimal cutpoint values and associated accuracy measures with their conffdence intervals, the graphical output includes the receiver operating characteristic (ROC) and predictive ROC curves. An illustration of the use of OptimalCutpoints is provided, using a real biomedical dataset.},
author = {L{\'{o}}pez-Rat{\'{o}}n, M{\'{o}}nica and Rodr{\'{i}}guez-{\'{A}}lvarez, Mar{\'{i}}a Xos{\'{e}} and Cadarso-Su{\'{a}}rez, Carmen and Gude-Sampedro, Francisco},
doi = {10.18637/jss.v061.i08},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\'{o}}pez-Rat{\'{o}}n et al. - 2014 - Optimalcutpoints An R package for selecting optimal cutpoints in diagnostic tests.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Accuracy measures,Diagnostic tests,Optimal cutpoint,R,ROC curve},
month = {nov},
number = {8},
pages = {1--36},
publisher = {American Statistical Association},
title = {{Optimalcutpoints: An R package for selecting optimal cutpoints in diagnostic tests}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v061i08/v61i08.pdf https://www.jstatsoft.org/index.php/jss/article/view/v061i08},
volume = {61},
year = {2014}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random forests.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classification,Ensemble,Regression},
month = {oct},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
url = {https://link.springer.com/article/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@article{Cohen1960,
abstract = {A coefficient of interjudge agreement for nominal scales, formula-omitted, is presented. It is directly interpretable as the pro-portion of joint judgments in which there is agreement, after chance agreement is excluded. Its upper limit is +1.00, and its lower limit falls between zero and -1.00, depending on the distribution of judgments by the two judges. The maximum value which x can take for any given problem is given, and the implications of this value to the question of agreement discussed. An interesting characteristic of x is its identity with 0 in the dichotomous case when the judges give the same marginal distributions. Finally, its standard error and techniques for estimation and hypothesis testing are presented. {\textcopyright} 1960, Sage Publications. All rights reserved.},
author = {Cohen, Jacob},
doi = {10.1177/001316446002000104},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {1},
pages = {37--46},
title = {{A Coefficient of Agreement for Nominal Scales}},
volume = {20},
year = {1960}
}
@article{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M. and Lee, Su In},
eprint = {1705.07874},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Lee - 2017 - A Unified Approach to Interpreting Model Predictions.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {may},
pages = {4766--4775},
publisher = {Neural information processing systems foundation},
title = {{A Unified Approach to Interpreting Model Predictions}},
url = {https://arxiv.org/abs/1705.07874v2},
volume = {2017-Decem},
year = {2017}
}
@article{Sachs2017,
abstract = {Plots of the receiver operating characteristic (ROC) curve are ubiquitous in medical research. Designed to simultaneously display the operating characteristics at every possible value of a continuous diagnostic test, ROC curves are used in oncology to evaluate screening, diagnostic, prognostic and predictive biomarkers. I reviewed a sample of ROC curve plots from the major oncology journals in order to assess current trends in usage and design elements. My review suggests that ROC curve plots are often ineffective as statistical charts and that poor design obscures the relevant information the chart is intended to display. I describe my new R package that was created to address the shortcomings of existing tools. The package has functions to create informative ROC curve plots, with sensible defaults and a simple interface, for use in print or as an interactive web-based plot. A web application was developed to reach a broader audience of scientists who do not use R.},
author = {Sachs, Michael C.},
doi = {10.18637/jss.v079.c02},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sachs - 2017 - Plotroc A tool for plotting ROC curves.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Graphics,Interactive,Plots,ROC curves},
month = {aug},
number = {1},
pages = {1--19},
publisher = {American Statistical Association},
title = {{Plotroc: A tool for plotting ROC curves}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v079c02/v79c02.pdf https://www.jstatsoft.org/index.php/jss/article/view/v079c02},
volume = {79},
year = {2017}
}
@misc{Vaughan2022,
abstract = {Implementations of the family of map() functions from 'purrr' that can be resolved using any 'future'-supported backend, e.g. parallel on the local machine or distributed on a compute cluster.},
author = {Vaughan, Davis and Dancho, Matt and RStudio},
month = {aug},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{furrr: Apply Mapping Functions in Parallel using Futures}},
url = {https://cran.r-project.org/package=furrr},
year = {2022}
}
@article{Covert2020,
abstract = {The Shapley value concept from cooperative game theory has become a popular
technique for interpreting ML models, but efficiently estimating these values
remains challenging, particularly in the model-agnostic setting. Here, we
revisit the idea of estimating Shapley values via linear regression to
understand and improve upon this approach. By analyzing the original KernelSHAP
alongside a newly proposed unbiased version, we develop techniques to detect
its convergence and calculate uncertainty estimates. We also find that the
original version incurs a negligible increase in bias in exchange for
significantly lower variance, and we propose a variance reduction technique
that further accelerates the convergence of both estimators. Finally, we
develop a version of KernelSHAP for stochastic cooperative games that yields
fast new estimators for two global explanation methods.},
archivePrefix = {arXiv},
arxivId = {2012.01536},
author = {Covert, Ian and Lee, Su In},
eprint = {2012.01536},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Covert, Lee - 2020 - Improving KernelSHAP Practical Shapley Value Estimation via Linear Regression.pdf:pdf},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
month = {dec},
pages = {3457--3465},
publisher = {ML Research Press},
title = {{Improving KernelSHAP: Practical Shapley Value Estimation via Linear Regression}},
url = {https://arxiv.org/abs/2012.01536v3},
volume = {130},
year = {2020}
}
@misc{Deane-Mayer2019,
abstract = {Description Functions for creating ensembles of caret models: caretList() and caretStack(). caretList() is a convenience function for fitting multiple caret::train() models to the same dataset. caretStack() will make linear or non-linear combinations of these models, using a caret::train() model as a meta-model, and caretEnsemble() will make a robust linear combination of models using a GLM. Depends R (>= 3.2.0)},
author = {Deane-Mayer, Zachary A and Knowles, Jared E},
booktitle = {R package version 2.0.1},
keywords = {MASS,Suggests caTools,caret License MIT + file LICENSE VignetteBuilder k,datatable,digest,e1071,gbm,ggplot2,glmnet,gridExtra,ipred,kernlab,klaR,knitr,lattice,lintr,mlbench,nnet,pROC,pbapply,plyr,randomForest,rmarkdown Imports methods,rpart,testthat},
month = {dec},
publisher = {Comprehensive R Archive Network (CRAN)},
title = {{caretEnsemble: Ensembles of Caret Models}},
url = {https://cran.r-project.org/package=caretEnsemble},
year = {2019}
}
@misc{Mayer2023a,
abstract = {Efficient implementation of Kernel SHAP, see Lundberg and Lee (2017) <https://dl.acm.org/doi/10.5555/3295222.3295230>, and Covert and Lee (2021) <http://proceedings.mlr.press/v130/covert21a>. For models with up to eight features, the results are exact regarding the selected background data. Otherwise, an almost exact hybrid algorithm involving iterative sampling is used. The package plays well together with meta-learning packages like 'tidymodels', 'caret' or 'mlr3'. Visualizations can be done using the R package 'shapviz'.},
author = {Mayer, Michael and Watson, David and Biecek, Przemyslaw},
title = {{kernelshap: Kernel SHAP}},
url = {https://cran.r-project.org/web/packages/kernelshap/index.html},
year = {2023}
}
@article{Brier1950,
abstract = {Two methods of solving the balance equation are outlined. Both methods have been used successfully on a daily operational basis at the Joint Numerical Weather Prediction Unit for a period of more than a year. Solutions were on the operational grid of 30 x 34 points spaced at 381-km. intervals.},
author = {Brier, Glenn W.},
doi = {10.1175/1520-0493(1950)078<0001:vofeit>2.0.co;2},
issn = {0027-0644},
journal = {Monthly Weather Review},
number = {1},
pages = {1--3},
title = {{VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY}},
url = {https://ui.adsabs.harvard.edu/abs/1950MWRv...78....1B/abstract},
volume = {78},
year = {1950}
}
@misc{Barnier2022,
abstract = {HTML formats and templates for 'rmarkdown' documents, with some extra features such as automatic table of contents, lightboxed figures, dynamic crosstab helper.},
author = {Barnier, Julien},
title = {{rmdformats: HTML Output Formats and Templates for 'rmarkdown' Documents}},
url = {https://cran.r-project.org/web/packages/rmdformats/index.html},
year = {2022}
}
@article{Hufner2022,
abstract = {Background: Coronavirus Disease-19 (COVID-19) convalescents are at risk of developing a de novo mental health disorder or worsening of a pre-existing one. COVID-19 outpatients have been less well characterized than their hospitalized counterparts. The objectives of our study were to identify indicators for poor mental health following COVID-19 outpatient management and to identify high-risk individuals. Methods: We conducted a binational online survey study with adult non-hospitalized COVID-19 convalescents (Austria/AT: n = 1,157, Italy/IT: n = 893). Primary endpoints were positive screening for depression and anxiety (Patient Health Questionnaire; PHQ-4) and self-perceived overall mental health (OMH) and quality of life (QoL) rated with 4 point Likert scales. Psychosocial stress was surveyed with a modified PHQ stress module. Associations of the mental health and QoL with socio-demographic, COVID-19 course, and recovery variables were assessed by multi-parameter Random Forest and Poisson modeling. Mental health risk subsets were defined by self-organizing maps (SOMs) and hierarchical clustering algorithms. The survey analyses are publicly available (https://im2-ibk.shinyapps.io/mental_health_dashboard/). Results: Depression and/or anxiety before infection was reported by 4.6% (IT)/6% (AT) of participants. At a median of 79 days (AT)/96 days (IT) post-COVID-19 onset, 12.4% (AT)/19.3% (IT) of subjects were screened positive for anxiety and 17.3% (AT)/23.2% (IT) for depression. Over one-fifth of the respondents rated their OMH (AT: 21.8%, IT: 24.1%) or QoL (AT: 20.3%, IT: 25.9%) as fair or poor. Psychosocial stress, physical performance loss, high numbers of acute and sub-acute COVID-19 complaints, and the presence of acute and sub-acute neurocognitive symptoms (impaired concentration, confusion, and forgetfulness) were the strongest correlates of deteriorating mental health and poor QoL. In clustering analysis, these variables defined subsets with a particularly high propensity of post-COVID-19 mental health impairment and decreased QoL. Pre-existing depression or anxiety (DA) was associated with an increased symptom burden during acute COVID-19 and recovery. Conclusion: Our study revealed a bidirectional relationship between COVID-19 symptoms and mental health. We put forward specific acute symptoms of the disease as “red flags” of mental health deterioration, which should prompt general practitioners to identify non-hospitalized COVID-19 patients who may benefit from early psychological and psychiatric intervention. Clinical Trial Registration: [ClinicalTrials.gov], identifier [NCT04661462].},
author = {H{\"{u}}fner, Katharina and Tymoszuk, Piotr and Ausserhofer, Dietmar and Sahanic, Sabina and Pizzini, Alex and Rass, Verena and Galffy, Matyas and B{\"{o}}hm, Anna and Kurz, Katharina and Sonnweber, Thomas and Tancevski, Ivan and Kiechl, Stefan and Huber, Andreas and Plagg, Barbara and Wiedermann, Christian J. and Bellmann-Weiler, Rosa and Bachler, Herbert and Weiss, G{\"{u}}nter and Piccoliori, Giuliano and Helbok, Raimund and Loeffler-Ragg, Judith and Sperner-Unterweger, Barbara},
doi = {10.3389/fmed.2022.792881},
issn = {2296858X},
journal = {Frontiers in Medicine},
keywords = {COVID-19,SARS-CoV-2,anxiety,depression,long COVID,machine learning,mental stress,neurocognitive},
month = {mar},
pmid = {35360744},
publisher = {Front Med (Lausanne)},
title = {{Who Is at Risk of Poor Mental Health Following Coronavirus Disease-19 Outpatient Management?}},
url = {https://pubmed.ncbi.nlm.nih.gov/35360744/},
volume = {9},
year = {2022}
}
@article{Karatzoglou2004,
abstract = {kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.},
author = {Karatzoglou, Alexandros and Hornik, Kurt and Smola, Alex and Zeileis, Achim},
doi = {10.18637/JSS.V011.I09},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karatzoglou et al. - 2004 - kernlab - An S4 Package for Kernel Methods in R.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {Clustering,Kernel methods,Quadratic programming,R,Ranking,S4,Support vector machines},
month = {nov},
pages = {1--20},
publisher = {American Statistical Association},
title = {{kernlab - An S4 Package for Kernel Methods in R}},
url = {https://www.jstatsoft.org/index.php/jss/article/view/v011i09},
volume = {11},
year = {2004}
}
@misc{Folashade2022,
abstract = {Provides a parallel backend for the %dopar% function using the parallel package.},
author = {Folashade, Daniel and {Microsoft Corporation} and Weston, Steve and Tenenbaum, Dan},
title = {{doParallel: Foreach Parallel Adaptor for the 'parallel' Package}},
url = {https://cran.r-project.org/web/packages/doParallel/index.html},
year = {2022}
}
@book{Wickham2016,
address = {New York},
author = {Wickham, Hadley.},
edition = {1},
isbn = {978-3-319-24277-4},
publisher = {Springer-Verlag},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {https://ggplot2.tidyverse.org},
year = {2016}
}
@article{Friedman2002,
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current "pseudo'-residuals by least squares at each iteration. The ...},
author = {Friedman, Jerome H.},
doi = {10.1016/S0167-9473(01)00065-2},
issn = {01679473},
journal = {Computational Statistics & Data Analysis},
month = {feb},
number = {4},
pages = {367--378},
publisher = {
		Elsevier Science Publishers B. V.
		PUB568
		Amsterdam, The Netherlands, The Netherlands
	},
title = {{Stochastic gradient boosting}},
url = {https://dl.acm.org/doi/10.1016/S0167-9473%2801%2900065-2},
volume = {38},
year = {2002}
}
@article{Rice2005,
abstract = {In order to facilitate comparisons across follow-up studies that have used different measures of effect size, we provide a table of effect size equivalencies for the three most common measures: ROC area (AUC), Cohen's d, and r. We outline why AUC is the preferred measure of predictive or diagnostic accuracy in forensic psychology or psychiatry, and we urge researchers and practitioners to use numbers rather than verbal labels to characterize effect sizes. {\textcopyright} 2005 American Psychology-Law Society/Division 41 of the American Psychological Association.},
author = {Rice, Marnie E. and Harris, Grant T.},
doi = {10.1007/S10979-005-6832-7},
issn = {01477307},
journal = {Law and Human Behavior},
keywords = {Effect size,Predictive accuracy,ROC area,Risk assessment},
month = {oct},
number = {5},
pages = {615--620},
pmid = {16254746},
title = {{Comparing effect sizes in follow-up studies: ROC area, Cohen's d, and r}},
volume = {29},
year = {2005}
}
@article{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
author = {Friedman, Jerome H.},
doi = {10.1214/AOS/1013203451},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman - 2001 - Greedy function approximation A gradient boosting machine.pdf:pdf},
issn = {0090-5364},
journal = {https://doi.org/10.1214/aos/1013203451},
keywords = {62-02,62-07,62-08,62G08,62H30,68T10,Function estimation,boosting,decision trees,robust nonparametric regression},
month = {oct},
number = {5},
pages = {1189--1232},
publisher = {Institute of Mathematical Statistics},
title = {{Greedy function approximation: A gradient boosting machine.}},
url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-appro},
volume = {29},
year = {2001}
}
@article{Sonnweber2022,
abstract = {Background: The optimal procedures to prevent, identify, monitor, and treat long-term pulmonary sequelae of COVID-19 are elusive. Here, we characterized the kinetics of respiratory and symptom recovery following COVID-19. Methods: We conducted a longitudinal, multicenter observational study in ambulatory and hospitalized COVID-19 patients recruited in early 2020 (n = 145). Pulmonary computed tomography (CT) and lung function (LF) readouts, symptom prevalence, and clinical and laboratory parameters were collected during acute COVID-19 and at 60, 100, and 180 days follow-up visits. Recovery kinetics and risk factors were investigated by logistic regression. Classification of clinical features and participants was accomplished by unsupervised and semi-supervised multiparameter clustering and machine learning. Results: At the 6-month follow-up, 49% of participants reported persistent symptoms. The frequency of structural lung CT abnormalities ranged from 18% in the mild outpatient cases to 76% in the intensive care unit (ICU) convalescents. Prevalence of impaired LF ranged from 14% in the mild outpatient cases to 50% in the ICU survivors. Incomplete radiological lung recovery was associated with increased anti-S1/S2 antibody titer, IL-6, and CRP levels at the early follow-up. We demonstrated that the risk of perturbed pulmonary recovery could be robustly estimated at early follow-up by clustering and machine learning classifiers employing solely non-CT and non-LF parameters. Conclusions: The severity of acute COVID-19 and protracted systemic inflammation is strongly linked to persistent structural and functional lung abnormality. Automated screening of multiparameter health record data may assist in the prediction of incomplete pulmonary recovery and optimize COVID-19 follow-up management. Funding: The State of Tyrol (GZ 71934), Boehringer Ingelheim/Investigator initiated study (IIS 1199-0424).},
author = {Sonnweber, Thomas and Tymoszuk, Piotr and Sahanic, Sabina and Boehm, Anna and Pizzini, Alex and Luger, Anna and Schwabl, Christoph and Nairz, Manfred and Grubwieser, Philipp and Kurz, Katharina and Koppelst{\"{a}}tter, Sabine and Aichner, Magdalena and Puchner, Bernhard and Egger, Alexander and Hoermann, Gregor and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Widmann, Gerlig and Tancevski, Ivan and L{\"{o}}ffler-Ragg, Judith},
doi = {10.7554/ELIFE.72500},
issn = {2050084X},
journal = {eLife},
keywords = {Judith L{\"{o}}ffler-Ragg,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Piotr Tymoszuk,PubMed Abstract,Thomas Sonnweber,doi:10.7554/eLife.72500,pmid:35131031},
month = {feb},
pmid = {35131031},
publisher = {Elife},
title = {{Investigating phenotypes of pulmonary COVID-19 recovery: A longitudinal observational prospective multicenter trial}},
url = {https://pubmed.ncbi.nlm.nih.gov/35131031/},
volume = {11},
year = {2022}
}
@article{Natekin2013,
abstract = {Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed. {\textcopyright} 2013 Natekin and Knoll.},
author = {Natekin, Alexey and Knoll, Alois},
doi = {10.3389/FNBOT.2013.00021/BIBTEX},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Natekin, Knoll - 2013 - Gradient boosting machines, a tutorial.pdf:pdf},
issn = {16625218},
journal = {Frontiers in Neurorobotics},
keywords = {Boosting,Classification,Gradient boosting,Machine learning,Regression,Robotic control,Text classification},
month = {dec},
number = {DEC},
pages = {63623},
publisher = {Frontiers Research Foundation},
title = {{Gradient boosting machines, a tutorial}},
volume = {7},
year = {2013}
}
@article{Hufner2023,
author = {H{\"{u}}fner, Katharina and Tymoszuk, Piotr and Sahanic, Sabina and Luger, Anna and Boehm, Anna and Pizzini, Alex and Schwabl, Christoph and Koppelst{\"{a}}tter, Sabine and Kurz, Katharina and Asshoff, Malte and Mosheimer-Feistritzer, Birgit and Pfeifer, Bernhard and Rass, Verena and Schroll, Andrea and Iglseder, Sarah and Egger, Alexander and W{\"{o}}ll, Ewald and Weiss, G{\"{u}}nter and Helbok, Raimund and Widmann, Gerlig and Sonnweber, Thomas and Tancevski, Ivan and Sperner-Unterweger, Barbara and L{\"{o}}ffler-Ragg, Judith},
doi = {10.1016/J.JPSYCHORES.2023.111234},
file = {:C\:/Users/piotr/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{u}}fner et al. - 2023 - Persistent somatic symptoms are key to individual illness perception at one year after COVID-19 in a cross-sectio.pdf:pdf},
issn = {0022-3999},
journal = {Journal of Psychosomatic Research},
month = {jun},
pages = {111234},
publisher = {Elsevier},
title = {{Persistent somatic symptoms are key to individual illness perception at one year after COVID-19 in a cross-sectional analysis of a prospective cohort study}},
volume = {169},
year = {2023}
}
@article{Hothorn2006,
abstract = {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown thai the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed. {\textcopyright} 2006 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
doi = {10.1198/106186006X133933},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Multiple testing,Multivariate regression trees,Ordinal regression trees,Permutation tests,Variable selection},
number = {3},
pages = {651--674},
publisher = {Taylor & Francis},
title = {{Unbiased recursive partitioning: A conditional inference framework}},
url = {https://www.tandfonline.com/doi/abs/10.1198/106186006X133933},
volume = {15},
year = {2006}
}
@inproceedings{Weston1998,
author = {Weston, Jason and Watkins, Chris},
title = {{Multi-Class Support Vector Machines}},
year = {1998}
}
@book{Hastie1991,
abstract = {First edition. Scope and content: "Statistical Models in S extends the S language to fit and analyze a variety of statistical models, including analysis of variance, generalized linear models, additive models, local regression, and tree-based models. The contributions of the ten authors-most of whom work in the statistics research department at AT & T Bell Laboratories-represent results of research in both the computational and statistical aspects of modeling data."--Provided by publisher. Chapter 1 An Appetizer -- chapter 2 Statistical Models -- chapter 3 Data for Models -- chapter 4 Linear Models -- chapter 5 Analysis of Variance; Designed Experiments -- chapter 6 Generalized Linear Models -- chapter 7 Generalized Additive Models -- chapter 8 Local Regression Models -- chapter 9 Tree-Based Models -- chapter 10 Nonlinear Models.},
address = {London},
author = {Hastie, T. J.},
edition = {1},
isbn = {1351414224},
publisher = {Routledge},
title = {{Statistical Models in S.}},
year = {1991}
}
@misc{Kassambara2021,
author = {Kassambara, Alboukadel},
title = {{rstatix: Pipe-Friendly Framework for Basic Statistical Tests}},
url = {https://cran.r-project.org/package=rstatix},
year = {2021}
}
@article{Fasiolo2020,
abstract = {We propose a novel framework for fitting additive quantile regression models, which provides well-calibrated inference about the conditional quantiles and fast automatic estimation of the smoothing...},
archivePrefix = {arXiv},
arxivId = {1707.03307},
author = {Fasiolo, Matteo and Wood, Simon N. and Zaffran, Margaux and Nedellec, Rapha{\"{e}}l and Goude, Yannig},
doi = {10.1080/01621459.2020.1725521},
eprint = {1707.03307},
issn = {1537274X},
journal = {https://doi.org/10.1080/01621459.2020.1725521},
keywords = {Calibrated Bayes,Electricity load forecasting,Generalized additive models,Penalized regression splines,Quantile regression},
number = {535},
pages = {1402--1412},
publisher = {Taylor & Francis},
title = {{Fast Calibrated Additive Quantile Regression}},
url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1725521},
volume = {116},
year = {2020}
}
